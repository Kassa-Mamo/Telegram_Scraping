{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer is installed correctly.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Check if the resource exists\n",
    "nltk.data.find('tokenizers/punkt')\n",
    "print(\"Punkt tokenizer is installed correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\user\\Desktop\\10 Acedamy W5\\TELEGRAM_SCRAPPER\\venv\\Scripts\\python.exe\n",
      "Python version: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from telethon import TelegramClient\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename=\"scraper.log\", filemode=\"w\",\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Configurations\n",
    "DATA_FOLDER = r\"C:\\Users\\user\\Desktop\\10 Acedamy W5\\All Data\"\n",
    "CHANNELS_FILE = os.path.join(DATA_FOLDER, \"channels_to_crawl.csv\")\n",
    "SCRAPED_FILE = os.path.join(DATA_FOLDER, \"scraped_data.csv\")\n",
    "API_ID = 20173022  # Replace with your API ID\n",
    "API_HASH = 'bab4a3351ed7634a8c1a3f8767fcf75c'  # Replace with your API Hash\n",
    "\n",
    "\n",
    "def load_channels(file_path):\n",
    "    \"\"\"Load channel usernames from a CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            # Ensure each line is a list of strings, then we can access the correct column (1 index) and strip\n",
    "            return [line[1].strip() for line in csv.reader(f) if len(line) > 1]\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Channels file not found at {file_path}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "async def scrape_channel(client, channel, writer):\n",
    "    \"\"\"Scrape messages from a Telegram channel.\"\"\"\n",
    "    try:\n",
    "        async for message in client.iter_messages(channel, limit=100):\n",
    "            writer.writerow([channel, message.sender_id, message.date, message.text])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping channel {channel}: {e}\")\n",
    "\n",
    "\n",
    "async def scrape_telegram_data():\n",
    "    \"\"\"Main function to scrape Telegram data.\"\"\"\n",
    "    logging.info(\"Starting Telegram scraper...\")\n",
    "    channels = load_channels(CHANNELS_FILE)\n",
    "    if not channels:\n",
    "        logging.error(\"No channels to scrape.\")\n",
    "        return\n",
    "\n",
    "    async with TelegramClient('scraper', API_ID, API_HASH) as client:\n",
    "        with open(SCRAPED_FILE, 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerow(['channel', 'sender', 'timestamp', 'message'])\n",
    "            for channel in channels:\n",
    "                logging.info(f\"Scraping channel: {channel}\")\n",
    "                await scrape_channel(client, channel, writer)\n",
    "\n",
    "    logging.info(\"Scraping completed.\")\n",
    "\n",
    "\n",
    "# Use await for environments with an active event loop\n",
    "if __name__ == \"__main__\":\n",
    "    await scrape_telegram_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data saved in CoNLL format to labeled_data.conll\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the entity types for labeling\n",
    "ENTITY_TYPES = {\n",
    "    \"B-Product\": \"Product entity\",\n",
    "    \"I-Product\": \"Product entity continuation\",\n",
    "    \"B-LOC\": \"Location entity\",\n",
    "    \"I-LOC\": \"Location entity continuation\",\n",
    "    \"B-PRICE\": \"Price entity\",\n",
    "    \"I-PRICE\": \"Price entity continuation\",\n",
    "    \"O\": \"Other (no entity)\"\n",
    "}\n",
    "\n",
    "# Example dataset (replace this with your dataset)\n",
    "data = [\n",
    "    (\"Baby bottle እንግዲኛ አሁን ዋጋ 1000 ብር\", \"B-Product I-Product O B-PRICE I-PRICE O\"),\n",
    "    (\"Addis Abeba በፍቅር እንቆቅልሽ\", \"B-LOC I-LOC O B-Product I-Product\")\n",
    "]\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Message\", \"Label\"])\n",
    "\n",
    "# Function to save labeled data in CoNLL format\n",
    "def save_conll_format(df, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            message = row['Message']\n",
    "            labels = row['Label'].split()\n",
    "            message_tokens = message.split()\n",
    "\n",
    "            # Iterate over tokens and corresponding labels\n",
    "            for token, label in zip(message_tokens, labels):\n",
    "                f.write(f\"{token} {label}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")  # Blank line between sentences/messages\n",
    "\n",
    "    print(f\"Labeled data saved in CoNLL format to {file_path}\")\n",
    "\n",
    "# Save the labeled data to CoNLL format\n",
    "save_conll_format(df, \"labeled_data.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return False\n",
    "print(torch.__version__)          # Confirm PyTorch version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\10 Acedamy W5\\TELEGRAM_SCRAPPER\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 11 examples [00:00, 355.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              text\n",
      "0  Example_Group O\n",
      "1             is O\n",
      "2          being O\n",
      "3      protected O\n",
      "4             by O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset in CoNLL format\n",
    "labeled_data_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\10 Acedamy W5\\\\All Data\\\\labeled_telegram_product_price_location.txt\"\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": labeled_data_path})\n",
    "\n",
    "# Convert to DataFrame for inspection (optional)\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 111.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 35378, 4, 8999, 38, 2, 1, 1, 1, 1, 1], [0, 11062, 82772, 7, 621, 113138, 38, 2, 1, 1, 1]]\n",
      "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define a sample dataset\n",
    "data = {\"text\": [\"Hello, world!\", \"Transformers are powerful!\"]}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Define the tokenize_and_align_labels function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # 'examples' here is passed from the dataset.map() function\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],  # Accessing the 'text' key from the input examples\n",
    "        padding=\"max_length\",  # Pads all sequences to the same length\n",
    "        truncation=True,       # Truncates sequences longer than max_length\n",
    "        max_length=11,         # Set to the desired fixed length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to your dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Print results to verify tokenized output\n",
    "print(tokenized_dataset[\"input_ids\"])  # This will show the tokenized IDs\n",
    "print(tokenized_dataset[\"attention_mask\"])  # This will show the attention mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dummy labels for token classification (this would be task-specific)\n",
    "labels = [\n",
    "    [1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0],  # Example labels for the first sentence\n",
    "    [1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],  # Example labels for the second sentence\n",
    "]\n",
    "\n",
    "# Adding labels to the dataset\n",
    "dataset = dataset.add_column(\"labels\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained XLM-RoBERTa model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)  # Change num_labels for your task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 170.31 examples/s]\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21620\\2359476935.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.869913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.9460399945576986, metrics={'train_runtime': 24.4349, 'train_samples_per_second': 0.246, 'train_steps_per_second': 0.123, 'total_flos': 33683089572.0, 'train_loss': 0.9460399945576986, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Sample dataset\n",
    "data = {\"text\": [\"Hello, world!\", \"Transformers are powerful!\"]}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Add dummy labels to match the length of tokenized inputs (for testing purposes)\n",
    "labels = [\n",
    "    [1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0],  # Labels for the first sentence\n",
    "    [1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],  # Labels for the second sentence\n",
    "]\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenizing the text while padding to max_length and truncating if necessary\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",  # Ensures all sequences are padded to max_length\n",
    "        truncation=True,       # Truncates if longer than max_length\n",
    "        max_length=11,         # Set to your desired fixed length (11 in this case)\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = labels  # Add labels to the tokenized inputs\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Load pre-trained model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # You can split this into train and eval datasets\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 200.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the 'test_dataset' already loaded (similar to how you loaded the training dataset)\n",
    "# If not, load it from a file or create it as a Dataset object\n",
    "\n",
    "# Example: If you have a list of examples as input\n",
    "test_data = {\"text\": [\"Test sentence 1\", \"Test sentence 2\"]}\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Apply the same tokenization process to the test dataset\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Ensure that the evaluation dataset is tokenized and ready\n",
    "print(tokenized_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9102998375892639, 'eval_runtime': 0.7184, 'eval_samples_per_second': 2.784, 'eval_steps_per_second': 1.392, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the trainer\n",
    "eval_results = trainer.evaluate(tokenized_test_dataset)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('your_tokenizer_save_path\\\\tokenizer_config.json',\n",
       " 'your_tokenizer_save_path\\\\special_tokens_map.json',\n",
       " 'your_tokenizer_save_path\\\\sentencepiece.bpe.model',\n",
       " 'your_tokenizer_save_path\\\\added_tokens.json',\n",
       " 'your_tokenizer_save_path\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"your_model_save_path\")\n",
    "tokenizer.save_pretrained(\"your_tokenizer_save_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('your_tokenizer_save_path\\\\tokenizer_config.json',\n",
       " 'your_tokenizer_save_path\\\\special_tokens_map.json',\n",
       " 'your_tokenizer_save_path\\\\sentencepiece.bpe.model',\n",
       " 'your_tokenizer_save_path\\\\added_tokens.json',\n",
       " 'your_tokenizer_save_path\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"your_tokenizer_save_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 385.63 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\user\\Desktop\\10 Acedamy W5\\TELEGRAM_SCRAPPER\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21620\\2940384145.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.743370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.742265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.740112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7401124835014343, 'eval_runtime': 0.4306, 'eval_samples_per_second': 11.611, 'eval_steps_per_second': 2.322, 'epoch': 3.0}\n",
      "Predictions:  tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. Create a sample text classification dataset\n",
    "data = {\n",
    "    \"text\": [\"I love machine learning!\", \"This is a bad product.\", \"Great customer service.\", \"Terrible experience.\", \"Amazing quality!\"],\n",
    "    \"label\": [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
    "}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# 2. Tokenizer Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. Model Setup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 4. Define TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",        # Output directory\n",
    "    num_train_epochs=3,            # Number of epochs\n",
    "    per_device_train_batch_size=8, # Batch size per device during training\n",
    "    per_device_eval_batch_size=16, # Batch size for evaluation\n",
    "    warmup_steps=500,              # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,             # Strength of weight decay\n",
    "    logging_dir=\"./logs\",          # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",   # Evaluation strategy to use\n",
    "    save_strategy=\"epoch\",         # Save model every epoch\n",
    "    load_best_model_at_end=True    # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# 5. Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # You can replace this with a separate test dataset\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# 7. Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# 8. Save the Model and Tokenizer\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# 9. Test the Model (for inference)\n",
    "# Test data for prediction\n",
    "test_data = [\"I am very happy with this product!\", \"Worst purchase I've made.\"]\n",
    "encoded_test_data = tokenizer(test_data, padding=True, truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_test_data)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(\"Predictions: \", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interprating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Training Process\n",
    "The training process spans 3 epochs:\n",
    "\n",
    "Training loss is not logged, as indicated by No log for each epoch.\n",
    "Validation loss:\n",
    "Epoch 1: 0.743370\n",
    "Epoch 2: 0.742265\n",
    "Epoch 3: 0.740112\n",
    "The validation loss is gradually decreasing, which suggests that the model is learning.\n",
    "3. Evaluation Results\n",
    "After training, the evaluation results are:\n",
    "\n",
    "Eval Loss: 0.740112\n",
    "Eval Runtime: 0.4306 seconds\n",
    "Eval Samples/Second: 11.611\n",
    "Eval Steps/Second: 2.322\n",
    "This indicates the model has evaluated the test dataset quickly, but the eval loss is still relatively high, implying that further training or hyperparameter tuning may be required.\n",
    "\n",
    "4. Predictions\n",
    "css\n",
    "Copy\n",
    "Edit\n",
    "Predictions:  tensor([1, 1])\n",
    "The model predicted two samples, both classified as 1. Depending on your labels, this corresponds to one of your target classes.\n",
    "\n",
    "Recommendations:\n",
    "Fine-Tuning:\n",
    "\n",
    "If the validation loss isn't satisfactory, consider:\n",
    "Increasing the number of epochs.\n",
    "Adjusting the learning rate.\n",
    "Experimenting with batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
